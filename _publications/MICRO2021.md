---
title: "Distilling Bit-level Sparsity Parallelism for General Purpose Deep Learning Acceleration"
collection: publications
permalink: /publications/MICRO2021
venue: "The 54th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO â€™21)"
date: 2021-10-18
citation: ''
---

## Abstract

Along with the rapid evolution of deep neural networks, the ever- increasing complexity imposes formidable computation intensity to the hardware accelerator. In this paper, we propose a novel computing philosophy called â€œbit interleavingâ€ and the associate accelerator design called â€œBitletâ€ to maximally exploit the bit-level sparsity. Apart from existing bit-serial/parallel accelerators, Bitlet leverages the abundant â€œsparsity parallelismâ€ in the parameters to enforce the inference acceleration. Bitlet is versatile by supporting diverse precisions on a single platform, including floating-point 32 and fixed-point from 1ğ‘ to 24ğ‘. The versatility enables Bitlet feasible for both efficient inference and training. Empirical studies on 12 domain-specific deep learning applications highlight the following results: (1) up to 81Ã—/21Ã— energy efficiency improvement for training/inference over recent high performance GPUs; (2) up to 15Ã—/8Ã— higher speedup/efficiency over state-of-the-art fixed-point accelerators; (3) 1.5ğ‘šğ‘š2 area and scalable power consumption from 570ğ‘šğ‘Š (ğ‘“ğ‘™ğ‘œğ‘ğ‘¡32) to 432ğ‘šğ‘Š (16ğ‘) and 365ğ‘šğ‘Š (8ğ‘) @28ğ‘›ğ‘š TSMC; (4) highly configurable justified by abundant ablation and sensitivity studies.